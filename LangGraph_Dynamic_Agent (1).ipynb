{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CIGVKzwNWRxI",
        "outputId": "3016cfd8-f809-4b3f-c8ef-45ae7b502ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.74)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading transformers-4.55.3-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, transformers, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.2\n",
            "    Uninstalling transformers-4.55.2:\n",
            "      Successfully uninstalled transformers-4.55.2\n",
            "Successfully installed langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.3 ormsgpack-1.10.0 transformers-4.55.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U huggingface_hub transformers langgraph langchain-core\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e4cac1b7",
        "outputId": "d522ec52-778d-4efd-a2f3-492fbc2afeda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.12/dist-packages (from langchain-experimental) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community, langchain-experimental\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.27 langchain-experimental-0.3.4 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GEddiKZlWbNi",
        "outputId": "59abc352-f990-4a14-e2e6-fd025b2729cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aSXRecjWYMj"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import json\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "token = # put hugging face token here\n",
        "\n",
        "\n",
        "if token:\n",
        "    login(token=token, add_to_git_credential=False)\n",
        "else:\n",
        "    print(\"HF_TOKEN secret not found. Public models will still be searchable, but gated/private ones may fail.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RI6G-y9YQig"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Load catalogs\n",
        "# ----------------------\n",
        "with open(\"/content/catalog_utils.json\", \"r\") as f:\n",
        "    tools_catalog = json.load(f)\n",
        "with open(\"/content/catalog_agents2.json\", \"r\") as f:\n",
        "    agents_catalog = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1saxNMkGNsAH"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai8HZ1IeYeuN"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional # Import Optional\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "# ----------------------\n",
        "# LLM client\n",
        "# ----------------------\n",
        "client = InferenceClient(provider=\"novita\", api_key=token)\n",
        "\n",
        "# ----------------------\n",
        "# UNIFIED LLM CALLER\n",
        "# ----------------------\n",
        "def call_llm(prompt, user_content, few_shot_file=None, model=\"openai/gpt-oss-120b\", temperature=0):\n",
        "    \"\"\"Wrapper to call LLM with optional few-shot examples stored in JSON.\"\"\"\n",
        "    few_shots = []\n",
        "    if few_shot_file:\n",
        "        with open(few_shot_file, \"r\") as f:\n",
        "            few_shots = json.load(f)\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": prompt}] + few_shots + [{\"role\": \"user\", \"content\": user_content}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        stream=False\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "# ----------------------\n",
        "# LLM helper for JSON rewrite (clarification)\n",
        "# ----------------------\n",
        "def call_llm_json_rewrite(plan_json: Dict[str, Any], human_feedback: str,\n",
        "                          few_shot_file: str = None,\n",
        "                          model: str = \"openai/gpt-oss-120b\",\n",
        "                          temperature: float = 0.0) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Ask the LLM to rewrite the given plan JSON based on human feedback.\n",
        "    This function forces the model (via a system prompt) to return JSON only.\n",
        "    Returns parsed JSON (dict) or raises ValueError if parsing fails.\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": CLARIFICATION_REWRITE_PROMPT}]\n",
        "    if few_shot_file:\n",
        "        try:\n",
        "            with open(few_shot_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                few_shots = json.load(f)\n",
        "            messages += few_shots\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Original plan JSON:\\n{json.dumps(plan_json, indent=2)}\"})\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Human feedback (natural language):\\n{human_feedback}\\n\\nRewrite the plan JSON accordingly. Output JSON only.\"})\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        stream=False\n",
        "    )\n",
        "    text = resp.choices[0].message[\"content\"]\n",
        "\n",
        "    # Parse JSON: best-effort\n",
        "    try:\n",
        "        parsed = json.loads(text)\n",
        "        return parsed\n",
        "    except Exception as e:\n",
        "        # If parsing failed, raise so caller can handle (e.g., prompt human to rephrase)\n",
        "        raise ValueError(f\"LLM did not return valid JSON. Error: {e}\\nRaw output:\\n{text}\")\n",
        "\n",
        "# Custom LLM wrapper class\n",
        "from langchain.llms.base import LLM\n",
        "class CustomLLM(LLM):\n",
        "    model: str = \"openai/gpt-oss-120b\"\n",
        "    temperature: float = 0\n",
        "    few_shot_file: Optional[str] = None\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # For agent use, we typically don't need a separate system prompt\n",
        "        # as the agent framework handles that\n",
        "        return call_llm(\"\", prompt, self.few_shot_file, self.model, self.temperature)\n",
        "\n",
        "# Initialize the custom LLM\n",
        "llm = CustomLLM(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0,\n",
        "    few_shot_file=None  # Set this if you have few-shot examples\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeAV91nMP4Nl"
      },
      "outputs": [],
      "source": [
        "def parse_prompt(prompt: str) -> Dict[str, Optional[str]]:\n",
        "    result = {\n",
        "        \"audio_dir\": None,\n",
        "        \"ground_truth_csv\": None,\n",
        "        \"lang_code\": None\n",
        "    }\n",
        "    path_pattern = r\"['\\\"]?(/[^'\\\"]+/[^'\\\"]+/)['\\\"]?\"\n",
        "    csv_pattern = r\"['\\\"]?(/[^'\\\"]+\\.csv)['\\\"]?\"\n",
        "    lang_pattern = r\"\\b(bn|gu|hi|kn|ml|mr|pa|sd|si|ta|te|ur)\\b\"\n",
        "    audio_dir_match = re.search(path_pattern, prompt)\n",
        "    if audio_dir_match:\n",
        "        result[\"audio_dir\"] = audio_dir_match.group(1)\n",
        "    csv_match = re.search(csv_pattern, prompt)\n",
        "    if csv_match:\n",
        "        result[\"ground_truth_csv\"] = csv_match.group(1)\n",
        "    lang_match = re.search(lang_pattern, prompt, re.IGNORECASE)\n",
        "    if lang_match:\n",
        "        result[\"lang_code\"] = lang_match.group(1).lower()\n",
        "    if not result[\"audio_dir\"] and not result[\"ground_truth_csv\"]:\n",
        "        llm_prompt = f\"\"\"Extract the following from the prompt:\n",
        "        1. Audio directory path (e.g., /path/to/audio/)\n",
        "        2. Ground truth CSV path (e.g., /path/to/file.csv)\n",
        "        3. Language code (e.g., hi, te)\n",
        "        If any are unclear, return None for that field.\n",
        "        Return a JSON object with keys 'audio_dir', 'ground_truth_csv', 'lang_code'.\n",
        "        Prompt:\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = call_llm(llm_prompt, prompt)    #llm.invoke(llm_prompt)\n",
        "            content = response.strip() # Use response from call_llm\n",
        "            if content.startswith('```json'):                                                                       #markdown format correction\n",
        "                content = content.replace('```json', '').replace('```', '').strip()\n",
        "            llm_result = json.loads(content)\n",
        "            for key in result:\n",
        "                if not result[key]:\n",
        "                    result[key] = llm_result.get(key)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"LLM prompt parsing failed: {e}\")                                                         # LLM failsafe\n",
        "            if audio_dir_match:\n",
        "                result[\"audio_dir\"] = audio_dir_match.group(1)\n",
        "            elif \"Audio_dir\" in prompt:\n",
        "                audio_dir_match = re.search(r\"Audio_dir\\s*=\\s*['\\\"]?(/[^'\\\"]+/)['\\\"]?\", prompt)\n",
        "                if audio_dir_match:\n",
        "                    result[\"audio_dir\"] = audio_dir_match.group(1)\n",
        "\n",
        "    if result[\"audio_dir\"] and not os.path.isdir(result[\"audio_dir\"]):\n",
        "        logging.error(f\"Invalid audio directory: {result['audio_dir']}\")\n",
        "        result[\"audio_dir\"] = None\n",
        "    if result[\"ground_truth_csv\"] and not os.path.isfile(result[\"ground_truth_csv\"]):\n",
        "        logging.error(f\"Invalid ground truth CSV: {result['ground_truth_csv']}\")\n",
        "        result[\"ground_truth_csv\"] = None\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u4I3vACUNunO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoTokenizer\n",
        "\n",
        "def transcribe_audio(audio_path, source_lang):\n",
        "    model_ids = {\n",
        "        \"Hindi\": \"ai4bharat/indicwav2vec-hindi\",\n",
        "        \"Tamil\": \"Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\",\n",
        "        \"Sanskrit\": \"addy88/wav2vec2-sanskrit-stt\",\n",
        "        \"Marathi\": \"ai4bharat/indicwav2vec-marathi\",\n",
        "        \"Telugu\": \"ai4bharat/indicwav2vec-telugu\",\n",
        "    }\n",
        "    if source_lang not in model_ids:\n",
        "        raise ValueError(f\"Unsupported language for ai4bharat/indicwav2vec2: {source_lang}\")\n",
        "    model_id = model_ids[source_lang]\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
        "    print(f\"Transcribing {audio_path} using {model_id}\")\n",
        "    audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = processor(audio_array, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        emission = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    transcript_words = transcription[0].strip().split()\n",
        "    print(\"TRANSCRIPT:\", transcript_words)\n",
        "    return \" \".join(transcript_words)\n",
        "\n",
        "\n",
        "def transcribe_folder_to_csv(folder_path: str, source_language: str):\n",
        "    output_path = os.path.join(folder_path, \"indicconf_hypothesis.csv\")\n",
        "    if os.path.exists(output_path):\n",
        "        logging.info(f\"Skipping transcription: File already exists at {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "    results = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        audio_path = os.path.join(folder_path, filename)\n",
        "        file_size = os.path.getsize(audio_path)\n",
        "        if file_size == 0:\n",
        "            logging.warning(f\"Skipping {filename}: File size is 0 bytes\")\n",
        "            continue\n",
        "\n",
        "        delete_after = False\n",
        "\n",
        "        if filename.lower().endswith(\".mp3\"):\n",
        "            try:\n",
        "                wav_filename = os.path.splitext(filename)[0] + \".wav\"\n",
        "                wav_path = os.path.join(folder_path, wav_filename)\n",
        "                audio = AudioSegment.from_mp3(audio_path)\n",
        "                audio.export(wav_path, format=\"wav\")\n",
        "                logging.info(f\"Converted {filename} to {wav_filename}\")\n",
        "                audio_path = wav_path\n",
        "                transcribe_filename = filename\n",
        "                delete_after = True\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to convert {filename} to .wav: {e}\")\n",
        "                continue\n",
        "\n",
        "        elif not filename.lower().endswith(\".wav\"):\n",
        "            logging.warning(f\"Skipping {filename}: Not a .wav or .mp3 file\")\n",
        "            continue\n",
        "        else:\n",
        "            transcribe_filename = filename\n",
        "\n",
        "        try:\n",
        "            hypothesis = transcribe_audio(audio_path, source_language)\n",
        "            results.append({\n",
        "                \"Filename\": transcribe_filename,\n",
        "                \"Indiconformer_Hypothesis\": hypothesis\n",
        "            })\n",
        "            logging.info(f\"Transcribed {transcribe_filename}: {hypothesis[:50]}...\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to transcribe {transcribe_filename}: {e}\")\n",
        "        finally:\n",
        "            if delete_after:\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    logging.info(f\"Deleted temporary file: {audio_path}\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to delete temporary .wav file {audio_path}: {e}\")\n",
        "\n",
        "    if not results:\n",
        "        logging.error(\"No valid transcriptions generated\")\n",
        "        return \"Error: No valid transcriptions\"\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    logging.info(f\"Transcriptions saved to {output_path}\")\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfXcjFfcYp-V"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f-EdIGsYnON"
      },
      "outputs": [],
      "source": [
        "TASK_DECOMPOSITION_PROMPT = \"\"\"\n",
        "You are a task decomposition system that maps a user's request about an audio dataset to:\n",
        "1. The list of EXISTING_TASKS (from a fixed predefined agent library, outputting function names)\n",
        "2. Any NEW_TASKS not covered by the existing library\n",
        "\n",
        "Your job:\n",
        "- Read the user’s request\n",
        "- Select **all** predefined agents whose meaning applies, regardless of exact wording\n",
        "- Add any new tasks that are not already covered by the predefined list\n",
        "\n",
        "## OUTPUT FORMAT\n",
        "Respond ONLY with a single JSON object that matches this schema exactly:\n",
        "{\n",
        "  \"EXISTING_TASKS\": [\"agent_function_name\", \"agent_function_name\", ...],\n",
        "  \"NEW_TASKS\": [\n",
        "    {\n",
        "      \"name\": \"string\",\n",
        "      \"description\": \"string\"\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "No extra text, comments, or explanations — only valid JSON.\n",
        "\n",
        "## SEMANTIC DEFINITIONS FOR EXISTING AGENTS\n",
        "Always select an agent if the request matches the meaning below, even if different words are used.\n",
        "\n",
        "\"transcription_func\": Transcription — Creating a written transcript of spoken content in the audio.\n",
        "\"num_speaker_func\": Speaker identification/diarization — Determining who is speaking, separating speech by speaker, counting speakers, or labeling speaker turns.\n",
        "\"transcript_quality_agent\": Transcript quality assessment — Evaluating transcription accuracy, completeness, and formatting.\n",
        "\"character_agent\": Character/graphene calculation — Counting characters or graphemes in the transcript.\n",
        "\"vocab_agent\": Vocabulary calculation — Counting and listing unique words in the transcript.\n",
        "\"language_verification_agent\": Language verification — Checking if the transcript matches the expected language.\n",
        "\"audio_length_agent\": Audio length calculation — Determining the total duration of audio files.\n",
        "\"silence_vad_func\": Silence detection — Identifying periods of no speech in the audio using VAD.\n",
        "\"english_word_count_agent\": English word counter — Counting English words in the transcript.\n",
        "\"ctc_score_agent\": CTC score calculation — Computing CTC-based alignment score between audio and transcript.\n",
        "\"upsampling_agent\": Upsampling check — Detecting artificially upsampled audio.\n",
        "\"valid_speaker_agent\": Speaker validity — Checking if speakers are new or previously known.\n",
        "\"domain_checker_agent\": Domain detection — Determining the speech dataset’s domain (e.g., medical, legal).\n",
        "\"audio_transcript_matching_agent\": Forced alignment — Mapping transcript text to corresponding audio segments.\n",
        "\"language_identification_indiclid_agent\": Language identification — Detecting the language(s) spoken in the audio.\n",
        "\"normalization_remove_tags_agent\": Transcript normalization — Removing HTML/XML tags and formatting from transcriptions.\n",
        "\"llm_score_agent\": Coherence/fluency scoring — Using LLM-as-a-Judge to score transcript fluency (1–10 scale).\n",
        "\"transliteration_agent\": Transliteration — Converting Roman script words to native script.\n",
        "\"corruption_agent\": Audio corruption detection — Detecting distortion, clipping, or other signal issues.\n",
        "\"extension_agent\": Audio format verification — Checking that files have correct extensions (e.g., .wav).\n",
        "\"sample_rate_agent\": Sample rate verification — Ensuring audio sample rates match requirements.\n",
        "\"speaker_duration_agent\": Per-speaker duration measurement — Calculating total speaking time for each speaker.\n",
        "\"utterance_duplicate_checker_agent\": Utterance duplicate check — Detecting duplicate transcriptions.\n",
        "\"wer_computation_agent\": WER computation — Calculating Word Error Rate between reference and hypothesis transcripts.\n",
        "\n",
        "## RULES\n",
        "- Always match based on meaning, not exact wording.\n",
        "- If a request includes multiple actions, include **all applicable agents**.\n",
        "- Do not reword existing agent names; only output their function names in EXISTING_TASKS.\n",
        "- Any requirement not covered above must be added to NEW_TASKS.\n",
        "- NEW_TASKS must have a concise `name` and a clear `description`.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "TOOL_PLANNER_SYSTEM_PROMPT = \"\"\"\n",
        "You are a Tool Planning Agent.\n",
        "\n",
        "# Inputs Provided to You:\n",
        "1. A catalog of existing agents and tool functions with their:\n",
        "   - descriptions, inputs, outputs, and dependencies.\n",
        "2. A list of tasks that must be mapped into tool/agent usage.\n",
        "3. The PipelineState schema (a state dictionary in LangGraph) that defines available state variables.\n",
        "\n",
        "# Important Convention:\n",
        "- Every tool or agent function must follow this signature:\n",
        "    def tool_name(state: PipelineState) -> PipelineState\n",
        "\n",
        "- Tools **do not take individual parameters**. They always receive the entire state dictionary and must:\n",
        "  1. Read the required fields from the state (e.g., `audio_dir = state.get(\"audio_dir\")`).\n",
        "  2. Perform the operation.\n",
        "  3. Write results back into the state using the correct field (e.g., `state[\"num_speakers\"] = value`).\n",
        "  4. Return the updated state.\n",
        "\n",
        "# PipelineState fields (shared memory between tools/agents):\n",
        "- audio_dir: str → Path to folder containing audio files\n",
        "- ground_truth_csv: str → Path to ground-truth CSV file if provided\n",
        "- transc_csv: str → Path to transcript CSV file created by transcript_func\n",
        "- test_transc_csv: str → Path to test transcript CSV file for validating new tools/agents\n",
        "- test_audio_dir: str → Path to test audio directory for validating new tools/agents\n",
        "- lang_code: str → Language code for processing\n",
        "- user_request: str → Raw natural language request from the user\n",
        "- task_decomposition: dict → Structured breakdown of tasks\n",
        "- task_list_for_planner: list[str] → Flattened list of tasks passed to planner\n",
        "- tool_plan: dict → JSON representation of tool/agent plan\n",
        "- clarification_round: int → Current clarification round\n",
        "- clarification_history: list[dict] → Log of clarification interactions\n",
        "- clarification_done: bool → Whether clarification is finished\n",
        "- human_feedback: str → Freeform feedback text from user\n",
        "\n",
        "# For each task you must decide:\n",
        "- Can it be fulfilled with an existing tool/agent?\n",
        "- Does an existing tool/agent need modification (small changes in I/O, additional checks)?\n",
        "- Or must a new tool/agent be created?\n",
        "- If a new tool/agent is proposed, you MUST decide if it is Achievable or Not Achievable:\n",
        "  - **Achievable = \"Yes\"** → provide technical justification (libraries).\n",
        "  - **Achievable = \"No\"** → provide reason + specify the missing capability (e.g., requires publicly available models, requires raw audio sentiment analysis, multimodal pretraining, unavailable model).\n",
        "  - If new tasks involves loading publicly available models , they SHOULD NOT be classified as ACHIEVABLE.\n",
        "\n",
        "\n",
        "# JSON Output Format (MANDATORY):\n",
        "{\n",
        "  \"PLAN\": [\n",
        "    {\n",
        "      \"task\": \"string - copy of the task description\",\n",
        "      \"use_existing\": [\"ToolName1\", \"ToolName2\"],\n",
        "      \"modify_existing\": [\n",
        "        {\n",
        "          \"tool\": \"ToolName\",\n",
        "          \"modification_description\": \"Concrete explanation of what needs to be changed\"\n",
        "        }\n",
        "      ],\n",
        "      \"create_new\": [\n",
        "        {\n",
        "          \"name\": \"string - proposed tool/agent name\",\n",
        "          \"description\": \"Detailed description of functionality and role\",\n",
        "          \"Achievable\" : \"Justification as to why it is `achievable` or `not achievable`\"\n",
        "          \"input_spec\": \"PipelineState - uses fields: [field1, field2]\" ,\n",
        "          \"output_spec\": \"PipelineState - updates fields: [field3, field4]\" ,\n",
        "          \"tool_dependencies\": [\"list\",\"of\",\"functions\",\"or\",\"libraries\",\"needed\"],\n",
        "          \"agent_dependencies\": [\"list\",\"of\",\"agents\",\"needed\",\"if\",\"any\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Rules (STRICT):\n",
        "1. Always describe I/O in terms of **PipelineState fields** if the new task is achievable:\n",
        "   - input_spec must list which fields are *read* from state.\n",
        "   - output_spec must list which fields are *written* to state.\n",
        "\n",
        "   Example:\n",
        "   \"input_spec\": \"PipelineState - uses fields: [audio_dir]\",\n",
        "   \"output_spec\": \"PipelineState - updates fields: [num_speakers]\"\n",
        "\n",
        "2. Be precise in `modification_description`:\n",
        "   - Example: \"Modify transcript_func to also read lang_code from PipelineState and handle multilingual input.\"\n",
        "\n",
        "3. Each task must have at least one of: `use_existing`, `modify_existing`, or `create_new`.\n",
        "\n",
        "4. `tool_dependencies`: only functions/modules/LLM utilities required for implementation.\n",
        "   `agent_dependencies`: only existing higher-level agents that this tool relies on.\n",
        "\n",
        "5. If a task involves evaluation, scoring, or subjective judgment prefer LLM: (example sentiment on text)\n",
        "   - First check if a scoring tool exists.\n",
        "   - If not, create a new scoring agent that wraps an LLM function.\n",
        "   - Always state: \"Scoring is performed by an LLM with a well-defined prompt.\"\n",
        "\n",
        "6. Multiple tasks → multiple objects in the \"PLAN\" list.\n",
        "   Do not merge unrelated tasks.\n",
        "\n",
        "# Example:\n",
        "{\n",
        "  \"PLAN\": [\n",
        "    {\n",
        "      \"task\": \"Count number of speakers in audio\",\n",
        "      \"use_existing\": [],\n",
        "      \"modify_existing\": [],\n",
        "      \"create_new\": [\n",
        "        {\n",
        "          \"name\": \"num_speaker_func\",\n",
        "          \"description\": \"Counts the number of distinct speakers in the audio files located in audio_dir.\",\n",
        "          \"input_spec\": \"PipelineState - uses fields: [audio_dir]\",\n",
        "          \"output_spec\": \"PipelineState - updates fields: [num_speakers]\",\n",
        "          \"tool_dependencies\": [\"pyannote.audio\"],\n",
        "          \"agent_dependencies\": []\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# TOOL_PLANNER_SYSTEM_PROMPT = \"\"\"\n",
        "# You are a Tool Planning Agent.\n",
        "\n",
        "# # You are given:\n",
        "# 1. A catalog of existing agents and tools functions with their descriptions, inputs, outputs, and dependencies.\n",
        "# 2. A list of tasks to perform.\n",
        "# 3. The PipelineState schema that defines available state variables in the LangGraph pipeline.\n",
        "\n",
        "# # PipelineState fields:\n",
        "# - audio_dir: str → Path to folder containing audio files\n",
        "# - ground_truth_csv: str → Path to ground-truth CSV file if provided\n",
        "# - transc_csv: str → Path to transcript CSV file created by transcript_func\n",
        "# - test_transc_csv: str → Path to test transcript CSV file for validating new tools/agents\n",
        "# - test_audio_dir: str → Path to test audio directory for validating new tools/agents\n",
        "# - lang_code: str → Language code for processing\n",
        "# - user_request: str → Raw natural language request from the user\n",
        "# - task_decomposition: dict → Structured breakdown of tasks\n",
        "# - task_list_for_planner: list[str] → Flattened list of tasks passed to planner\n",
        "# - tool_plan: dict → JSON representation of tool/agent plan\n",
        "# - clarification_round: int → Current clarification round\n",
        "# - clarification_history: list[dict] → Log of clarification interactions\n",
        "# - clarification_done: bool → Whether clarification is finished\n",
        "# - human_feedback: str → Freeform feedback text from user\n",
        "\n",
        "# # For each task, decide:\n",
        "# - If an existing tool/agent can be used as-is\n",
        "# - If an existing tool/agent can be modified to achieve it\n",
        "# - If a new tool/agent must be created\n",
        "\n",
        "# # Output JSON only in this format:\n",
        "# {\n",
        "#   \"PLAN\": [\n",
        "#     {\n",
        "#       \"task\": \"string\",\n",
        "#       \"use_existing\": [\"ToolName1\", \"ToolName2\"],\n",
        "#       \"modify_existing\": [\n",
        "#         {\n",
        "#           \"tool\": \"ToolName\",\n",
        "#           \"modification_description\": \"string\"\n",
        "#         }\n",
        "#       ],\n",
        "#       \"create_new\": [\n",
        "#         {\n",
        "#           \"name\": \"string\",\n",
        "#           \"description\": \"string\",\n",
        "#           \"input_spec\": \"string - must reference a PipelineState field when possible\",\n",
        "#           \"output_spec\": \"string - must reference a PipelineState field when possible\",\n",
        "#           \"tool_dependencies\": [\"list\",\"of\",\"strings\",\"of\",\"tool\",\"names\",\"from\",\"catalog\"],\n",
        "#           \"agent_dependencies\": [\"list\",\"of\",\"strings\",\"of\",\"agent\",\"names\",\"from\",\"catalog\"]\n",
        "#         }\n",
        "#       ]\n",
        "#     }\n",
        "#   ]\n",
        "# }\n",
        "\n",
        "# # Rules:\n",
        "# - Always prefer PipelineState fields for input_spec and output_spec (e.g., `\"string - Path to folder containing audio files (audio_dir) from PipelineState\"`).\n",
        "# - If no relevant PipelineState field exists, allow a general spec (e.g., \"string - User query\").\n",
        "# - Only reference tools and agents from the provided catalog unless creating new.\n",
        "# - Match dependencies based on functionality, not just wording similarity.\n",
        "# - input_spec and output_spec :\n",
        "#     - `\"type - meaning + PipelineState + field\"` when applicable. important to mention state name\n",
        "#     - `\"type - meaning \"`\n",
        "# - tool_dependencies: list only function names, modules, llm or libraries directly needed for the implementation.\n",
        "# - agent_dependencies: list existing agents that might be needed before/after implementing this tool/agent.\n",
        "# - If a task involves evaluation, scoring, grading, or subjective judgment (e.g., relevance, sentiment, fluency, coherence), check for a scoring tool in the catalog first.\n",
        "#   - If none exists, create a new LLM-based scoring agent that uses an existing LLM utility function as the primary dependency.\n",
        "#   - Always specify in the description that scoring is performed by an LLM with a well-defined prompt.\n",
        "\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSLYo0qJsZT_"
      },
      "outputs": [],
      "source": [
        "CLARIFICATION_REWRITE_PROMPT = \"\"\"\n",
        "You are a JSON rewrite assistant.\n",
        "\n",
        "You will be given:\n",
        "1) Original tool plan JSON.\n",
        "2) Natural language human feedback.\n",
        "\n",
        "Task:\n",
        "- Incorporate the human feedback into the original plan by modifying only the necessary parts.\n",
        "- Keep the exact JSON structure:\n",
        "{\n",
        "  \"PLAN\": [\n",
        "    {\n",
        "      \"task\": \"string\",\n",
        "      \"use_existing\": [\"ToolName1\", \"ToolName2\"],\n",
        "      \"modify_existing\": [\n",
        "        {\"tool\": \"ToolName\", \"modification_description\": \"string\"}\n",
        "      ],\n",
        "      \"create_new\": [\n",
        "        {\n",
        "          \"name\": \"string\",\n",
        "          \"description\": \"string\",\n",
        "          \"input_spec\": \"string\",\n",
        "          \"output_spec\": \"string\",\n",
        "          \"tool_dependencies\": [\"list\",\"of\",\"strings\"],\n",
        "          \"agent_dependencies\": [\"list\",\"of\",\"strings\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "Field Descriptions:\n",
        "- **task**: A short description of the specific task to be performed.\n",
        "- **use_existing**: List of existing tools that will be directly used without modification.\n",
        "- **modify_existing**: List of modifications to existing tools (tool name + description of change).\n",
        "- **create_new**: List of entirely new tools that need to be created. Each new tool requires:\n",
        "  - **name**: Unique name of the new tool.\n",
        "  - **description**: Purpose and functionality of the tool.\n",
        "  - **input_spec**: Description of input format and requirements.\n",
        "  - **output_spec**: Description of output format and expectations.\n",
        "  - **tool_dependencies**: List of other tools this tool depends on.\n",
        "  - **agent_dependencies**: List of agent processes or agents this tool depends on.\n",
        "\n",
        "**Important:**\n",
        "- Output **only valid JSON** (no extra text, no commentary).\n",
        "- Preserve unchanged entries unless the human feedback requires a change.\n",
        "- If feedback asks to merge or remove items, reflect that exactly in the JSON.\n",
        "- The three fields **use_existing, modify_existing, create_new** are exhaustive.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX_x3e3eT_sg"
      },
      "source": [
        "## Python Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBpHNEr8UBkb",
        "outputId": "79887099-d5a9-48cd-c28f-a0f25689aa7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-406648470.py:12: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "repl_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "tools = [repl_tool]\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMNSYM-zYuRO"
      },
      "source": [
        "## LangGraph Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG-GxJ-3b47Z"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any\n",
        "# ----------------------\n",
        "# State type\n",
        "# ----------------------\n",
        "class PipelineState(TypedDict, total=False):\n",
        "    audio_dir: str\n",
        "    ground_truth_csv: str\n",
        "    transc_csv: str\n",
        "    lang_code: str\n",
        "    user_request: str\n",
        "    task_decomposition: Dict[str, Any]\n",
        "    task_list_for_planner: List[str]\n",
        "    tool_plan: Dict[str, Any]\n",
        "    clarification_round: int\n",
        "    clarification_history: List[Dict[str, Any]]\n",
        "    clarification_done: bool\n",
        "    human_feedback: str  # Add this field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBfJBnGwYtod"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# LANGGRAPH NODES\n",
        "# ----------------------\n",
        "\n",
        "def task_decomposition_node(state):\n",
        "    \"\"\"Node 1: Decompose user request into tasks.\"\"\"\n",
        "    print(\"---Task Decomposition---\")\n",
        "    user_request = state[\"user_request\"]\n",
        "    print(f\"--------------------Calling LLM--------------------\")\n",
        "    task_decomp_output = call_llm(\n",
        "        TASK_DECOMPOSITION_PROMPT,\n",
        "        user_request,\n",
        "        few_shot_file=\"/content/task_decomposer.json\"\n",
        "    )\n",
        "    print(f\"--------------------LLM output--------------------\")\n",
        "    print(task_decomp_output)\n",
        "\n",
        "    tasks_data = json.loads(task_decomp_output)\n",
        "    task_list_for_planner = list(tasks_data[\"EXISTING_TASKS\"]) + [t[\"name\"] for t in tasks_data[\"NEW_TASKS\"]]\n",
        "\n",
        "    return {\n",
        "        \"task_decomposition\": tasks_data,\n",
        "        \"task_list_for_planner\": task_list_for_planner\n",
        "    }\n",
        "\n",
        "\n",
        "def tool_planning_node(state):\n",
        "    \"\"\"Node 2: Plan tools/agents based on tasks.\"\"\"\n",
        "    print(\"---Tool Planning---\")\n",
        "    tool_planner_input = json.dumps({\n",
        "        \"tools_catalog\": tools_catalog,\n",
        "        \"agents_catalog\": agents_catalog,\n",
        "        \"tasks\": state[\"task_list_for_planner\"]\n",
        "    }, indent=2)\n",
        "\n",
        "    print(f\"--------------------Calling LLM--------------------\")\n",
        "    tool_plan_output = call_llm(\n",
        "        TOOL_PLANNER_SYSTEM_PROMPT,\n",
        "        tool_planner_input,\n",
        "        few_shot_file=\"/content/tool_planner.json\"\n",
        "    )\n",
        "    raw = tool_plan_output.strip()\n",
        "    print(f\"Tool plan: {raw}\")\n",
        "\n",
        "    try:\n",
        "        tool_plan = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                tool_plan = json.loads(match.group(0))\n",
        "            except json.JSONDecodeError:\n",
        "                raise ValueError(f\"Extracted JSON block is invalid: {match.group(0)}\")\n",
        "\n",
        "    return {\"tool_plan\": tool_plan}\n",
        "\n",
        "# ----------------------\n",
        "# Node 3: Clarification (HITL) - CORRECTED VERSION\n",
        "# ----------------------\n",
        "\n",
        "def clarification_request_node(state):\n",
        "    \"\"\"\n",
        "    Ask human to review tool plan and provide feedback.\n",
        "    Uses interrupt() to pause execution and wait for human input.\n",
        "    \"\"\"\n",
        "    print(\"---Clarification Request---\")\n",
        "    current_plan = state.get(\"tool_plan\", {})\n",
        "    round_number = state.get(\"clarification_round\", 0) + 1\n",
        "\n",
        "    prompt_text = (\n",
        "        f\"=== Clarification Round {round_number} ===\\n\\n\"\n",
        "        \"Please review the current tool plan below and respond with:\\n\"\n",
        "        \"- 'approve' or 'ok' to accept the plan, OR\\n\"\n",
        "        \"- Natural language instructions to change the plan.\\n\\n\"\n",
        "        \"Current plan:\\n\"\n",
        "        f\"{json.dumps(current_plan, indent=2)}\\n\\n\"\n",
        "        \"Your feedback:\"\n",
        "    )\n",
        "\n",
        "    # Use interrupt() to pause and wait for human feedback\n",
        "    feedback = interrupt(prompt_text)\n",
        "\n",
        "    # Return the feedback to be stored in state\n",
        "    return {\n",
        "        \"human_feedback\": feedback,\n",
        "        \"clarification_round\": round_number\n",
        "    }\n",
        "\n",
        "\n",
        "def clarification_process_node(state):\n",
        "    \"\"\"\n",
        "    Process human feedback, update plan if needed.\n",
        "    \"\"\"\n",
        "    print(\"---Processing Clarification---\")\n",
        "    current_plan = copy.deepcopy(state.get(\"tool_plan\", {}))\n",
        "    feedback_text = (state.get(\"human_feedback\") or \"\").strip()\n",
        "    round_number = state.get(\"clarification_round\", 0)\n",
        "\n",
        "    # If approved → finish\n",
        "    if feedback_text.lower() in {\"ok\", \"approve\", \"approved\", \"yes\", \"looks good\"}:\n",
        "        history_entry = {\n",
        "            \"round\": round_number,\n",
        "            \"feedback\": feedback_text,\n",
        "            \"action\": \"approved\",\n",
        "            \"plan_before\": current_plan,\n",
        "            \"plan_after\": current_plan\n",
        "        }\n",
        "        clarification_history = state.get(\"clarification_history\", []) + [history_entry]\n",
        "        return {\n",
        "            \"clarification_history\": clarification_history,\n",
        "            \"clarification_done\": True\n",
        "        }\n",
        "\n",
        "    # Otherwise → rewrite plan with LLM\n",
        "    rewritten_plan = call_llm_json_rewrite(\n",
        "        plan_json=current_plan,\n",
        "        human_feedback=feedback_text,\n",
        "        few_shot_file=None,\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    history_entry = {\n",
        "        \"round\": round_number,\n",
        "        \"feedback\": feedback_text,\n",
        "        \"action\": \"rewritten\",\n",
        "        \"plan_before\": current_plan,\n",
        "        \"plan_after\": rewritten_plan\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"tool_plan\": rewritten_plan,\n",
        "        \"clarification_history\": state.get(\"clarification_history\", []) + [history_entry],\n",
        "        \"clarification_done\": False\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CiWGrgbajSe"
      },
      "outputs": [],
      "source": [
        "def transcription_func(state: PipelineState):\n",
        "    audio_dir = state.get('audio_dir')\n",
        "    if not audio_dir or not os.path.isdir(audio_dir):\n",
        "        logging.error(f\"Invalid audio directory for transcription: {audio_dir}\")\n",
        "        return {\"A\": \"Error: Invalid audio directory\"}\n",
        "    logging.info(\"Running Transcription\")\n",
        "    result = transcribe_folder_to_csv(audio_dir, source_language=\"Hindi\")\n",
        "    return {\"transc_csv\": result} #, \"audio_dir\": audio_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmDkIW-7Y3hd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import copy\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.constants import START\n",
        "from langgraph.types import Command, interrupt\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "def clarification_condition(state):\n",
        "    \"\"\"Decide whether to continue or end clarification loop.\"\"\"\n",
        "    if state.get(\"clarification_done\", False) or state.get(\"clarification_round\", 0) >= 3:\n",
        "        return END\n",
        "    return \"clarification_request\"\n",
        "\n",
        "\n",
        "# Build state graph\n",
        "graph = StateGraph(PipelineState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"task_decomposition\", task_decomposition_node)\n",
        "graph.add_node(\"tool_planning\", tool_planning_node)\n",
        "graph.add_node(\"clarification_request\", clarification_request_node)\n",
        "graph.add_node(\"clarification_process\", clarification_process_node)\n",
        "\n",
        "# Add edges\n",
        "graph.add_edge(START, \"task_decomposition\")\n",
        "graph.add_edge(\"task_decomposition\", \"tool_planning\")\n",
        "graph.add_edge(\"tool_planning\", \"clarification_request\")\n",
        "graph.add_edge(\"clarification_request\", \"clarification_process\")\n",
        "\n",
        "# Add conditional edge for clarification loop\n",
        "graph.add_conditional_edges(\"clarification_process\", clarification_condition)\n",
        "\n",
        "# CRITICAL: Add checkpointer for human-in-the-loop to work\n",
        "memory = InMemorySaver()\n",
        "app = graph.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p0di-NqZI3I"
      },
      "outputs": [],
      "source": [
        "def run_pipeline_with_hitl(user_request: str):\n",
        "    \"\"\"Run the pipeline with proper human-in-the-loop handling.\"\"\"\n",
        "\n",
        "    # Create thread configuration\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"pipeline_thread_1\"}}\n",
        "\n",
        "    # Initial state\n",
        "    initial_state = {\"user_request\": user_request, \"clarification_round\": 0}\n",
        "\n",
        "    print(\"Starting pipeline execution...\")\n",
        "\n",
        "    # Stream the graph execution\n",
        "    for event in app.stream(initial_state, thread_config, stream_mode=\"updates\"):\n",
        "        print(f\"Event: {event}\")\n",
        "\n",
        "        # Check if we hit an interrupt\n",
        "        if \"__interrupt__\" in event:\n",
        "            interrupt_data = event[\"__interrupt__\"][0]\n",
        "            print(f\"\\n HUMAN INPUT REQUIRED:\")\n",
        "            print(f\"Prompt: {interrupt_data.value}\")\n",
        "\n",
        "            # Get human feedback\n",
        "            human_feedback = input(\"\\nYour feedback: \").strip()\n",
        "\n",
        "            # Resume execution with the feedback\n",
        "            print(f\"\\n Resuming with feedback: {human_feedback}\")\n",
        "\n",
        "            # Continue streaming with the human feedback\n",
        "            for resume_event in app.stream(\n",
        "                Command(resume=human_feedback),\n",
        "                thread_config,\n",
        "                stream_mode=\"updates\"\n",
        "            ):\n",
        "                print(f\"Resume Event: {resume_event}\")\n",
        "\n",
        "                # Handle nested interrupts (if clarification loops)\n",
        "                if \"__interrupt__\" in resume_event:\n",
        "                    nested_interrupt = resume_event[\"__interrupt__\"][0]\n",
        "                    print(f\"\\n ADDITIONAL INPUT REQUIRED:\")\n",
        "                    print(f\"Prompt: {nested_interrupt.value}\")\n",
        "\n",
        "                    nested_feedback = input(\"\\nYour feedback: \").strip()\n",
        "                    print(f\"\\n Resuming with feedback: {nested_feedback}\")\n",
        "\n",
        "                    # Continue with nested feedback\n",
        "                    for final_event in app.stream(\n",
        "                        Command(resume=nested_feedback),\n",
        "                        thread_config,\n",
        "                        stream_mode=\"updates\"\n",
        "                    ):\n",
        "                        print(f\"Final Event: {final_event}\")\n",
        "\n",
        "            break  # Exit the main loop after handling interrupt\n",
        "\n",
        "    # Get final state\n",
        "    final_state = app.get_state(thread_config)\n",
        "    print(\"\\n=== Final State ===\")\n",
        "    print(json.dumps(final_state.values, indent=2))\n",
        "\n",
        "    return final_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHDiFd0v4cx8"
      },
      "outputs": [],
      "source": [
        "user_request = (\"I have audio file directory at /content/audios and i need to generate  transcript of each audio, and accent of the audio .\") # sentiment of the transcript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mQ84wat-xfo",
        "outputId": "900026ac-8dd7-4382-d592-745e343bd66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting pipeline execution...\n",
            "---Task Decomposition---\n",
            "--------------------Calling LLM--------------------\n",
            "--------------------LLM output--------------------\n",
            "{\n",
            "  \"EXISTING_TASKS\": [\"transcription_func\"],\n",
            "  \"NEW_TASKS\": [\n",
            "    {\n",
            "      \"name\": \"accent_detection_agent\",\n",
            "      \"description\": \"Detect and label the accent(s) present in each audio file or speaker.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "Event: {'task_decomposition': {'task_decomposition': {'EXISTING_TASKS': ['transcription_func'], 'NEW_TASKS': [{'name': 'accent_detection_agent', 'description': 'Detect and label the accent(s) present in each audio file or speaker.'}]}, 'task_list_for_planner': ['transcription_func', 'accent_detection_agent']}}\n",
            "---Tool Planning---\n",
            "--------------------Calling LLM--------------------\n",
            "Tool plan: {\n",
            "  \"PLAN\": [\n",
            "    {\n",
            "      \"task\": \"transcription_func\",\n",
            "      \"use_existing\": [\"transcription_func\"],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": []\n",
            "    },\n",
            "    {\n",
            "      \"task\": \"accent_detection_agent\",\n",
            "      \"use_existing\": [],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": [\n",
            "        {\n",
            "          \"name\": \"accent_detection_agent\",\n",
            "          \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\n",
            "          \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\n",
            "          \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]\",\n",
            "          \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\n",
            "          \"tool_dependencies\": [\"torch\", \"torchaudio\", \"speechbrain (or similar accent classification library)\"],\n",
            "          \"agent_dependencies\": []\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "Event: {'tool_planning': {'tool_plan': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}}}\n",
            "---Clarification Request---\n",
            "Event: {'__interrupt__': (Interrupt(value='=== Clarification Round 1 ===\\n\\nPlease review the current tool plan below and respond with:\\n- \\'approve\\' or \\'ok\\' to accept the plan, OR\\n- Natural language instructions to change the plan.\\n\\nCurrent plan:\\n{\\n  \"PLAN\": [\\n    {\\n      \"task\": \"transcription_func\",\\n      \"use_existing\": [\\n        \"transcription_func\"\\n      ],\\n      \"modify_existing\": [],\\n      \"create_new\": []\\n    },\\n    {\\n      \"task\": \"accent_detection_agent\",\\n      \"use_existing\": [],\\n      \"modify_existing\": [],\\n      \"create_new\": [\\n        {\\n          \"name\": \"accent_detection_agent\",\\n          \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\\n          \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\\n          \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\\\u2011specific models)]\",\\n          \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\\n          \"tool_dependencies\": [\\n            \"torch\",\\n            \"torchaudio\",\\n            \"speechbrain (or similar accent classification library)\"\\n          ],\\n          \"agent_dependencies\": []\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\nYour feedback:', id='e071555253ec0903cd01f54c38de8fec'),)}\n",
            "\n",
            " HUMAN INPUT REQUIRED:\n",
            "Prompt: === Clarification Round 1 ===\n",
            "\n",
            "Please review the current tool plan below and respond with:\n",
            "- 'approve' or 'ok' to accept the plan, OR\n",
            "- Natural language instructions to change the plan.\n",
            "\n",
            "Current plan:\n",
            "{\n",
            "  \"PLAN\": [\n",
            "    {\n",
            "      \"task\": \"transcription_func\",\n",
            "      \"use_existing\": [\n",
            "        \"transcription_func\"\n",
            "      ],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": []\n",
            "    },\n",
            "    {\n",
            "      \"task\": \"accent_detection_agent\",\n",
            "      \"use_existing\": [],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": [\n",
            "        {\n",
            "          \"name\": \"accent_detection_agent\",\n",
            "          \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\n",
            "          \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\n",
            "          \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "          \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\n",
            "          \"tool_dependencies\": [\n",
            "            \"torch\",\n",
            "            \"torchaudio\",\n",
            "            \"speechbrain (or similar accent classification library)\"\n",
            "          ],\n",
            "          \"agent_dependencies\": []\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Your feedback:\n",
            "\n",
            "Your feedback: ok\n",
            "\n",
            " Resuming with feedback: ok\n",
            "---Clarification Request---\n",
            "Resume Event: {'clarification_request': {'human_feedback': 'ok', 'clarification_round': 1}}\n",
            "---Processing Clarification---\n",
            "Resume Event: {'clarification_process': {'clarification_history': [{'round': 1, 'feedback': 'ok', 'action': 'approved', 'plan_before': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low‑level accent‑classification utility (built on SpeechBrain or a fine‑tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\", 'Achievable': 'Yes – Accent classification can be performed with publicly available models (e.g., SpeechBrain’s `accent-id` recipe, or a Wav2Vec2 model fine‑tuned on accent‑labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool‑agent pattern.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain', 'pandas', 'os', 'logging'], 'agent_dependencies': []}]}]}, 'plan_after': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low‑level accent‑classification utility (built on SpeechBrain or a fine‑tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\", 'Achievable': 'Yes – Accent classification can be performed with publicly available models (e.g., SpeechBrain’s `accent-id` recipe, or a Wav2Vec2 model fine‑tuned on accent‑labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool‑agent pattern.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain', 'pandas', 'os', 'logging'], 'agent_dependencies': []}]}]}}, {'round': 1, 'feedback': 'ok', 'action': 'approved', 'plan_before': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}, 'plan_after': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}}], 'clarification_done': True}}\n",
            "\n",
            "=== Final State ===\n",
            "{\n",
            "  \"user_request\": \"I have audio file directory at /content/audios and i need to generate  transcript of each audio, and accent of the audio .\",\n",
            "  \"task_decomposition\": {\n",
            "    \"EXISTING_TASKS\": [\n",
            "      \"transcription_func\"\n",
            "    ],\n",
            "    \"NEW_TASKS\": [\n",
            "      {\n",
            "        \"name\": \"accent_detection_agent\",\n",
            "        \"description\": \"Detect and label the accent(s) present in each audio file or speaker.\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"task_list_for_planner\": [\n",
            "    \"transcription_func\",\n",
            "    \"accent_detection_agent\"\n",
            "  ],\n",
            "  \"tool_plan\": {\n",
            "    \"PLAN\": [\n",
            "      {\n",
            "        \"task\": \"transcription_func\",\n",
            "        \"use_existing\": [\n",
            "          \"transcription_func\"\n",
            "        ],\n",
            "        \"modify_existing\": [],\n",
            "        \"create_new\": []\n",
            "      },\n",
            "      {\n",
            "        \"task\": \"accent_detection_agent\",\n",
            "        \"use_existing\": [],\n",
            "        \"modify_existing\": [],\n",
            "        \"create_new\": [\n",
            "          {\n",
            "            \"name\": \"accent_detection_agent\",\n",
            "            \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\n",
            "            \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\n",
            "            \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "            \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\n",
            "            \"tool_dependencies\": [\n",
            "              \"torch\",\n",
            "              \"torchaudio\",\n",
            "              \"speechbrain (or similar accent classification library)\"\n",
            "            ],\n",
            "            \"agent_dependencies\": []\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"clarification_round\": 1,\n",
            "  \"clarification_history\": [\n",
            "    {\n",
            "      \"round\": 1,\n",
            "      \"feedback\": \"ok\",\n",
            "      \"action\": \"approved\",\n",
            "      \"plan_before\": {\n",
            "        \"PLAN\": [\n",
            "          {\n",
            "            \"task\": \"transcription_func\",\n",
            "            \"use_existing\": [\n",
            "              \"transcription_func\"\n",
            "            ],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": []\n",
            "          },\n",
            "          {\n",
            "            \"task\": \"accent_detection_agent\",\n",
            "            \"use_existing\": [],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": [\n",
            "              {\n",
            "                \"name\": \"accent_detection_agent\",\n",
            "                \"description\": \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low\\u2011level accent\\u2011classification utility (built on SpeechBrain or a fine\\u2011tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\",\n",
            "                \"Achievable\": \"Yes \\u2013 Accent classification can be performed with publicly available models (e.g., SpeechBrain\\u2019s `accent-id` recipe, or a Wav2Vec2 model fine\\u2011tuned on accent\\u2011labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool\\u2011agent pattern.\",\n",
            "                \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "                \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)\",\n",
            "                \"tool_dependencies\": [\n",
            "                  \"torch\",\n",
            "                  \"torchaudio\",\n",
            "                  \"speechbrain\",\n",
            "                  \"pandas\",\n",
            "                  \"os\",\n",
            "                  \"logging\"\n",
            "                ],\n",
            "                \"agent_dependencies\": []\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      \"plan_after\": {\n",
            "        \"PLAN\": [\n",
            "          {\n",
            "            \"task\": \"transcription_func\",\n",
            "            \"use_existing\": [\n",
            "              \"transcription_func\"\n",
            "            ],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": []\n",
            "          },\n",
            "          {\n",
            "            \"task\": \"accent_detection_agent\",\n",
            "            \"use_existing\": [],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": [\n",
            "              {\n",
            "                \"name\": \"accent_detection_agent\",\n",
            "                \"description\": \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low\\u2011level accent\\u2011classification utility (built on SpeechBrain or a fine\\u2011tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\",\n",
            "                \"Achievable\": \"Yes \\u2013 Accent classification can be performed with publicly available models (e.g., SpeechBrain\\u2019s `accent-id` recipe, or a Wav2Vec2 model fine\\u2011tuned on accent\\u2011labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool\\u2011agent pattern.\",\n",
            "                \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "                \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)\",\n",
            "                \"tool_dependencies\": [\n",
            "                  \"torch\",\n",
            "                  \"torchaudio\",\n",
            "                  \"speechbrain\",\n",
            "                  \"pandas\",\n",
            "                  \"os\",\n",
            "                  \"logging\"\n",
            "                ],\n",
            "                \"agent_dependencies\": []\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"round\": 1,\n",
            "      \"feedback\": \"ok\",\n",
            "      \"action\": \"approved\",\n",
            "      \"plan_before\": {\n",
            "        \"PLAN\": [\n",
            "          {\n",
            "            \"task\": \"transcription_func\",\n",
            "            \"use_existing\": [\n",
            "              \"transcription_func\"\n",
            "            ],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": []\n",
            "          },\n",
            "          {\n",
            "            \"task\": \"accent_detection_agent\",\n",
            "            \"use_existing\": [],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": [\n",
            "              {\n",
            "                \"name\": \"accent_detection_agent\",\n",
            "                \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\n",
            "                \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\n",
            "                \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "                \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\n",
            "                \"tool_dependencies\": [\n",
            "                  \"torch\",\n",
            "                  \"torchaudio\",\n",
            "                  \"speechbrain (or similar accent classification library)\"\n",
            "                ],\n",
            "                \"agent_dependencies\": []\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      \"plan_after\": {\n",
            "        \"PLAN\": [\n",
            "          {\n",
            "            \"task\": \"transcription_func\",\n",
            "            \"use_existing\": [\n",
            "              \"transcription_func\"\n",
            "            ],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": []\n",
            "          },\n",
            "          {\n",
            "            \"task\": \"accent_detection_agent\",\n",
            "            \"use_existing\": [],\n",
            "            \"modify_existing\": [],\n",
            "            \"create_new\": [\n",
            "              {\n",
            "                \"name\": \"accent_detection_agent\",\n",
            "                \"description\": \"Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.\",\n",
            "                \"Achievable\": \"No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.\",\n",
            "                \"input_spec\": \"PipelineState - uses fields: [audio_dir, lang_code (optional for language\\u2011specific models)]\",\n",
            "                \"output_spec\": \"PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)\",\n",
            "                \"tool_dependencies\": [\n",
            "                  \"torch\",\n",
            "                  \"torchaudio\",\n",
            "                  \"speechbrain (or similar accent classification library)\"\n",
            "                ],\n",
            "                \"agent_dependencies\": []\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"clarification_done\": true,\n",
            "  \"human_feedback\": \"ok\"\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "StateSnapshot(values={'user_request': 'I have audio file directory at /content/audios and i need to generate  transcript of each audio, and accent of the audio .', 'task_decomposition': {'EXISTING_TASKS': ['transcription_func'], 'NEW_TASKS': [{'name': 'accent_detection_agent', 'description': 'Detect and label the accent(s) present in each audio file or speaker.'}]}, 'task_list_for_planner': ['transcription_func', 'accent_detection_agent'], 'tool_plan': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}, 'clarification_round': 1, 'clarification_history': [{'round': 1, 'feedback': 'ok', 'action': 'approved', 'plan_before': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low‑level accent‑classification utility (built on SpeechBrain or a fine‑tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\", 'Achievable': 'Yes – Accent classification can be performed with publicly available models (e.g., SpeechBrain’s `accent-id` recipe, or a Wav2Vec2 model fine‑tuned on accent‑labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool‑agent pattern.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain', 'pandas', 'os', 'logging'], 'agent_dependencies': []}]}]}, 'plan_after': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': \"Analyzes each audio file in the dataset to detect the speaker's accent (e.g., regional variations of Hindi, Tamil, Marathi, etc.). The agent reads the audio directory, calls a low‑level accent‑classification utility (built on SpeechBrain or a fine‑tuned Wav2Vec2 model), aggregates the results into a CSV with columns `filename, accent_label, confidence_score`, and writes the CSV path to PipelineState['accent_detection_output']. If any file cannot be processed, the agent records an error row and continues.\", 'Achievable': 'Yes – Accent classification can be performed with publicly available models (e.g., SpeechBrain’s `accent-id` recipe, or a Wav2Vec2 model fine‑tuned on accent‑labeled speech). Required libraries (torch, torchaudio, speechbrain, pandas) are installable via pip, and the inference code fits the existing tool‑agent pattern.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (string path to the generated CSV file)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain', 'pandas', 'os', 'logging'], 'agent_dependencies': []}]}]}}, {'round': 1, 'feedback': 'ok', 'action': 'approved', 'plan_before': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}, 'plan_after': {'PLAN': [{'task': 'transcription_func', 'use_existing': ['transcription_func'], 'modify_existing': [], 'create_new': []}, {'task': 'accent_detection_agent', 'use_existing': [], 'modify_existing': [], 'create_new': [{'name': 'accent_detection_agent', 'description': 'Detects speaker accent characteristics in each audio file of a speech dataset. The agent reads the audio directory, processes each file with an accent classification model, and writes a CSV summarizing filename, detected accent label, confidence score, and any model metadata.', 'Achievable': 'No - requires a specialized accent classification model and training data that are not part of the current codebase or listed dependencies; no publicly available model is integrated, making implementation infeasible at present.', 'input_spec': 'PipelineState - uses fields: [audio_dir, lang_code (optional for language‑specific models)]', 'output_spec': 'PipelineState - updates fields: [accent_detection_output] (path to CSV with columns: Filename, Accent_Label, Confidence, Model_Info)', 'tool_dependencies': ['torch', 'torchaudio', 'speechbrain (or similar accent classification library)'], 'agent_dependencies': []}]}]}}], 'clarification_done': True, 'human_feedback': 'ok'}, next=(), config={'configurable': {'thread_id': 'pipeline_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f07f4b7-574c-697e-800e-ffe874e0b29c'}}, metadata={'source': 'loop', 'step': 14, 'parents': {}}, created_at='2025-08-22T11:30:51.834793+00:00', parent_config={'configurable': {'thread_id': 'pipeline_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f07f4b7-5748-6b34-800d-1148fe14319a'}}, tasks=(), interrupts=())"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_pipeline_with_hitl(user_request)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwYzVO_9BCX4",
        "outputId": "cc02f16e-8b17-4788-ccb4-3a001616bc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current round: 2\n",
            "Tool plan: {\n",
            "  \"PLAN\": [\n",
            "    {\n",
            "      \"task\": \"transcription_func\",\n",
            "      \"use_existing\": [\n",
            "        \"transcription_func\"\n",
            "      ],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": []\n",
            "    },\n",
            "    {\n",
            "      \"task\": \"sentiment_analysis\",\n",
            "      \"use_existing\": [],\n",
            "      \"modify_existing\": [],\n",
            "      \"create_new\": [\n",
            "        {\n",
            "          \"name\": \"sentiment_analysis_agent\",\n",
            "          \"description\": \"Analyzes sentiment (positive, neutral, negative) of each transcription in the CSV produced by the transcription step. The agent reads the path stored in PipelineState['transc_csv'], loads the CSV, sends each transcript to an LLM with a sentiment\\u2011classification prompt, collects the LLM responses, adds a new column `sentiment` to the dataframe, writes the enriched CSV to a new file, and updates PipelineState['sentiment_output'] with the path of the generated CSV.\",\n",
            "          \"Achievable\": \"Sentiment can be reliably obtained by prompting an LLM (e.g., GPT\\u20114) with a clear instruction and parsing the returned label. The required libraries (pandas for CSV handling and the existing `llm` tool for LLM calls) are already available in the environment.\",\n",
            "          \"input_spec\": \"PipelineState - uses fields: [transc_csv]\",\n",
            "          \"output_spec\": \"PipelineState - updates fields: [sentiment_output]\",\n",
            "          \"tool_dependencies\": [\n",
            "            \"llm\",\n",
            "            \"pandas\"\n",
            "          ],\n",
            "          \"agent_dependencies\": []\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "thread_config = {\"configurable\": {\"thread_id\": \"pipeline_thread_1\"}}\n",
        "\n",
        "# Get current state\n",
        "state = app.get_state(thread_config)\n",
        "current_values = state.values\n",
        "\n",
        "# Access specific elements\n",
        "user_request = current_values.get(\"user_request\")\n",
        "tool_plan = current_values.get(\"tool_plan\")\n",
        "task_decomposition = current_values.get(\"task_decomposition\")\n",
        "clarification_round = current_values.get(\"clarification_round\", 0)\n",
        "clarification_done = current_values.get(\"clarification_done\", False)\n",
        "\n",
        "print(f\"Current round: {clarification_round}\")\n",
        "print(f\"Tool plan: {json.dumps(tool_plan, indent=2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9711vWweBP7r"
      },
      "outputs": [],
      "source": [
        "# workflow agent - > ask which tools to execute\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdLOODhjdflj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "def generate_agent_function_code(task_spec: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Generate a complete agent function using LLM for intelligent code generation.\n",
        "    \"\"\"\n",
        "\n",
        "    function_name = task_spec.get(\"name\")\n",
        "\n",
        "    # Use LLM to generate the entire function intelligently\n",
        "    generation_prompt = f\"\"\"\n",
        "You need to generate a Python function similar to this example pattern:\n",
        "\n",
        "```python\n",
        "def corruption_agent(state: PipelineState) -> PipelineState:\n",
        "    audio_dir = state.get('audio_dir')\n",
        "    if not audio_dir or not os.path.isdir(audio_dir):\n",
        "        logging.error(f\"Invalid audio directory for corruption check: {{audio_dir}}\")\n",
        "        return {{\"corruption_output\": \"Invalid: No audio directory\"}}\n",
        "\n",
        "    task_prompt = \\\"\\\"\\\"You are given a folder with audios at this path: {{audio_dir}}.\n",
        "    Write a Python script to do the following and then execute it using [python_repl]:\n",
        "    - Import the 'os' module.\n",
        "    - Create a CSV named 'corruption_check.csv' in the same directory.\n",
        "    - The CSV should have columns: Filename, Status.\n",
        "    - For each audio file in the folder:\n",
        "        - Use os.path.getsize(file_path) to get the file size.\n",
        "        - If the file size is 0 KB, mark its 'Status' column as 'corrupt'.\n",
        "        - If the file size is of .mp3 format, mark its 'Status' column as 'corrupt'.\n",
        "        - Otherwise, mark its 'Status' column as 'valid'.\n",
        "    - Save the CSV.\n",
        "    Finally, respond with \"Success\" if all files are valid, otherwise respond with \"Invalid\".\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    try:\n",
        "        response = agent.invoke(task_prompt)\n",
        "        return {{\"corruption_output\": response}}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Corruption check failed: {{e}}\")\n",
        "        return {{\"corruption_output\": f\"Error: {{e}}\"}}\n",
        "```\n",
        "\n",
        "Now generate a similar function for this task specification:\n",
        "- Function name: {function_name}\n",
        "- Description: {task_spec.get(\"description\")}\n",
        "- Input specification: {json.dumps(task_spec.get(\"input_spec\", {}), indent=2)}\n",
        "- Output specification: {task_spec.get(\"output_spec\", \"\")}\n",
        "- Dependencies: {task_spec.get(\"python_dependencies\", [])}\n",
        "- Error handling: {task_spec.get(\"error_handling\", \"\")}\n",
        "\n",
        "Requirements:\n",
        "1. PLease analyze the CSV and make no assumptions about column names\n",
        "2. Analyze the task description to determine the appropriate approach:\n",
        "  - If task requires AI analysis, judgment, classification, or content understanding → Use hybrid approach\n",
        "  - If task is purely computational, file operations, or data processing → Use python_repl only\n",
        "3. For hybrid approach tasks, structure the task_prompt with clear steps:\n",
        "  - \"Step 1: Use [python_repl] to [data operations]\"\n",
        "  - \"Step 2: For each item, make LLM calls yourself (NOT through python_repl) to [analyze/classify/judge]\"\n",
        "  - \"Step 3: Use [python_repl] to [save/write results]\"\n",
        "4. For python-only tasks, use the corruption_agent pattern with only [python_repl] instructions\n",
        "5. Extract appropriate state keys based on the input specification description\n",
        "6. Include proper validation for extracted state values (file/directory existence checks)\n",
        "7. Include comprehensive error handling and logging\n",
        "8. Return a dictionary with descriptive output key (function_name + \"_output\")\n",
        "9. Make the task_prompt extremely specific with exact step-by-step instructions, not generic descriptions\n",
        "10. Ensure the task_prompt is actionable and executable by the agent\n",
        "\n",
        "Generate ONLY the complete function code, no explanations.\n",
        "\"\"\"\n",
        "\n",
        "    function_code = call_llm(\"\", generation_prompt, model=\"openai/gpt-oss-120b\", temperature=0.1)\n",
        "    return function_code.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGxu54YVt3kk",
        "outputId": "2b159698-e92c-4949-b316-56dc00483395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "import os\n",
            "import logging\n",
            "\n",
            "def sentiment_analysis_agent(state: PipelineState) -> PipelineState:\n",
            "    transc_csv = state.get('transc_csv')\n",
            "    if not transc_csv or not os.path.isfile(transc_csv):\n",
            "        logging.error(f\"Invalid transcription CSV path for sentiment analysis: {transc_csv}\")\n",
            "        return {\"sentiment_analysis_output\": \"Invalid: No CSV file\"}\n",
            "\n",
            "    task_prompt = f\"\"\"You are given a CSV file at path: {transc_csv}. Perform the following steps using a hybrid approach:\n",
            "\n",
            "Step 1: Use [python_repl] to:\n",
            "- Import pandas as pd.\n",
            "- Load the CSV into a DataFrame named df.\n",
            "- Detect the column that contains transcription text:\n",
            "    * Look for the first column whose header contains the word \"transcript\" (case‑insensitive).\n",
            "    * If none is found, assume the first column after any index column.\n",
            "- Keep df in memory for the next step.\n",
            "\n",
            "Step 2: For each row in df, make a direct LLM call (do NOT use python_repl) with the prompt:\n",
            "\\\"\\\"\\\"Classify the sentiment of the following text as Positive, Neutral, or Negative. Respond with only one word (Positive/Neutral/Negative). Text: \\\"{{transcript}}\\\"\\\"\\\"\\\"\n",
            "Replace {{transcript}} with the actual transcript from the identified column.\n",
            "Collect the LLM response and assign it to a new column called \"sentiment\" for that row.\n",
            "\n",
            "Step 3: Use [python_repl] to:\n",
            "- Add the \"sentiment\" column to the original DataFrame.\n",
            "- Write the enriched DataFrame to a new CSV file in the same directory named \"sentiment_analysis.csv\".\n",
            "- Return the absolute path of the generated CSV as the final response.\n",
            "\n",
            "Finally, respond with the path returned from Step 3.\"\"\"\n",
            "    try:\n",
            "        response = agent.invoke(task_prompt)\n",
            "        return {\"sentiment_analysis_output\": response}\n",
            "    except Exception as e:\n",
            "        logging.error(f\"Sentiment analysis failed: {e}\")\n",
            "        return {\"sentiment_analysis_output\": f\"Error: {e}\"}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# EXAMPLE USAGE\n",
        "# ----------------------\n",
        "\n",
        "sentiment_task ={\n",
        "          \"name\": \"sentiment_analysis_agent\",\n",
        "          \"description\": \"Analyzes sentiment (positive, neutral, negative) of each transcription in the CSV produced by the transcription step. The agent reads the path stored in PipelineState['transc_csv'], loads the CSV, sends each transcript to an LLM with a sentiment\\u2011classification prompt, collects the LLM responses, adds a new column `sentiment` to the dataframe, writes the enriched CSV to a new file, and updates PipelineState['sentiment_output'] with the path of the generated CSV.\",\n",
        "          \"Achievable\": \"Sentiment can be reliably obtained by prompting an LLM (e.g., GPT\\u20114) with a clear instruction and parsing the returned label. The required libraries (pandas for CSV handling and the existing `llm` tool for LLM calls) are already available in the environment.\",\n",
        "          \"input_spec\": \"PipelineState - uses fields: [transc_csv]\",\n",
        "          \"output_spec\": \"PipelineState - updates fields: [sentiment_output]\",\n",
        "          \"tool_dependencies\": [\n",
        "            \"llm\",\n",
        "            \"pandas\"\n",
        "          ],\n",
        "          \"agent_dependencies\": []\n",
        "        }\n",
        "\n",
        "# Test validation state\n",
        "test_state = {\n",
        "    \"csv_path\": \"/path/to/test_transcript.csv\",\n",
        "    \"test_transcript_csv\": \"/path/to/test_transcript.csv\"\n",
        "}\n",
        "\n",
        "# Create and validate\n",
        "result = generate_agent_function_code(sentiment_task)\n",
        "print(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CHWmuQl00Rg"
      },
      "outputs": [],
      "source": [
        "# def extract_state_keys_from_spec(task_spec: Dict[str, Any]) -> list:\n",
        "#     \"\"\"\n",
        "#     Use LLM to intelligently extract state keys from input specification.\n",
        "#     \"\"\"\n",
        "\n",
        "#     input_spec = task_spec.get(\"input_spec\", {})\n",
        "\n",
        "#     extraction_prompt = f\"\"\"\n",
        "# Given this input specification for a task:\n",
        "# {json.dumps(input_spec, indent=2)}\n",
        "\n",
        "# And this task description:\n",
        "# {task_spec.get(\"description\", \"\")}\n",
        "\n",
        "# What are the specific state dictionary keys that this function would need to extract from the state?\n",
        "\n",
        "# For example:\n",
        "# - If it mentions \"csv_path\", return [\"csv_path\"]\n",
        "# - If it mentions \"audio directory\", return [\"audio_dir\"]\n",
        "# - If it mentions \"ground truth file\", return [\"ground_truth_csv\"]\n",
        "\n",
        "# Return ONLY a Python list of strings with the exact key names, nothing else.\n",
        "# Example format: [\"csv_path\", \"audio_dir\"]\n",
        "# \"\"\"\n",
        "\n",
        "#     response = call_llm(\"\", extraction_prompt, model=\"openai/gpt-oss-120b\", temperature=0)\n",
        "\n",
        "#     # Parse the response to extract the list\n",
        "#     try:\n",
        "#         # Try to evaluate the response as a Python list\n",
        "#         import ast\n",
        "#         state_keys = ast.literal_eval(response.strip())\n",
        "#         return state_keys if isinstance(state_keys, list) else []\n",
        "#     except:\n",
        "#         return []\n",
        "\n",
        "def validate_and_fix_function(function_code: str, task_spec: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Test the generated function with test data and fix if needed.\n",
        "    \"\"\"\n",
        "\n",
        "    function_name = task_spec.get(\"name\")\n",
        "\n",
        "    # First, try to execute the function definition\n",
        "    test_prompt = f\"\"\"\n",
        "First, execute this function definition:\n",
        "\n",
        "```python\n",
        "{function_code}\n",
        "```\n",
        "\n",
        "Now test it with this test state:\n",
        "```python\n",
        "test_state = {json.dumps(state, indent=2)}\n",
        "\n",
        "# Test the function\n",
        "try:\n",
        "    result = {function_name}(test_state)\n",
        "    print(\"Function executed successfully!\")\n",
        "    print(\"Result:\", result)\n",
        "    print(\"Result type:\", type(result))\n",
        "except Exception as e:\n",
        "    print(\"Function execution failed!\")\n",
        "    print(\"Error:\", str(e))\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "```\n",
        "\n",
        "If there are any errors, identify what went wrong and suggest fixes.\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"Testing function {function_name} with validation data...\")\n",
        "    test_result = agent.invoke(test_prompt)\n",
        "\n",
        "    # Check if there were errors\n",
        "    if \"❌\" in test_result or \"Error:\" in test_result:\n",
        "        print(\"🔧 Function needs fixing, generating corrected version...\")\n",
        "\n",
        "        # Use LLM to fix the function\n",
        "        fix_prompt = f\"\"\"\n",
        "The function had errors during testing. Here's the original function:\n",
        "\n",
        "```python\n",
        "{function_code}\n",
        "```\n",
        "\n",
        "Test result and errors:\n",
        "{test_result}\n",
        "\n",
        "Test state used:\n",
        "{json.dumps(state, indent=2)}\n",
        "\n",
        "Please generate a corrected version of the function that fixes these issues.\n",
        "Make sure to:\n",
        "1. Handle the specific errors that occurred\n",
        "2. Ensure the function works with the provided test state\n",
        "3. Follow the same pattern as the original\n",
        "4. Include proper error handling\n",
        "\n",
        "Generate ONLY the corrected complete function code.\n",
        "\"\"\"\n",
        "\n",
        "        corrected_code = call_llm(\"\", fix_prompt, model=\"openai/gpt-oss-20b\", temperature=0.1)\n",
        "\n",
        "        # Test the corrected function\n",
        "        corrected_test_prompt = f\"\"\"\n",
        "Execute the corrected function:\n",
        "\n",
        "```python\n",
        "{corrected_code}\n",
        "```\n",
        "\n",
        "Test it again:\n",
        "```python\n",
        "test_state = {json.dumps(state, indent=2)}\n",
        "try:\n",
        "    result = {function_name}(test_state)\n",
        "    print(\"✅ Corrected function works!\")\n",
        "    print(\"Result:\", result)\n",
        "except Exception as e:\n",
        "    print(\"❌ Still has errors:\")\n",
        "    print(\"Error:\", str(e))\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "        final_test_result = agent.invoke(corrected_test_prompt)\n",
        "\n",
        "        return {\n",
        "            \"function_code\": corrected_code,\n",
        "            \"original_code\": function_code,\n",
        "            \"test_result\": final_test_result,\n",
        "            \"needed_correction\": True,\n",
        "            \"success\": \"✅\" in final_test_result\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        return {\n",
        "            \"function_code\": function_code,\n",
        "            \"test_result\": test_result,\n",
        "            \"needed_correction\": False,\n",
        "            \"success\": True\n",
        "        }\n",
        "\n",
        "def create_and_validate_agent_function(task_spec: Dict[str, Any], validation_state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create an agent function and validate it with test data.\n",
        "    \"\"\"\n",
        "\n",
        "    function_name = task_spec.get(\"name\")\n",
        "    print(f\"🔨 Creating and validating agent function: {function_name}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Generate the function code using LLM\n",
        "        print(\"1️⃣ Generating function code...\")\n",
        "        function_code = generate_agent_function_code(task_spec)\n",
        "\n",
        "        print(\"Generated function preview:\")\n",
        "        print(\"=\"*50)\n",
        "        print(function_code[:500] + \"...\" if len(function_code) > 500 else function_code)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Step 2: Validate and fix the function\n",
        "        print(\"2️⃣ Validating function with test data...\")\n",
        "        validation_result = validate_and_fix_function(function_code, task_spec, validation_state)\n",
        "\n",
        "        if validation_result[\"success\"]:\n",
        "            print(f\"✅ Function {function_name} created and validated successfully!\")\n",
        "        else:\n",
        "            print(f\"❌ Function {function_name} validation failed\")\n",
        "\n",
        "        return {\n",
        "            \"function_name\": function_name,\n",
        "            \"task_spec\": task_spec,\n",
        "            **validation_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Failed to create function {function_name}: {str(e)}\"\n",
        "        print(f\"❌ {error_msg}\")\n",
        "\n",
        "        return {\n",
        "            \"function_name\": function_name,\n",
        "            \"error\": error_msg,\n",
        "            \"success\": False\n",
        "        }\n",
        "\n",
        "def dynamic_function_creation_and_validation_node(state):\n",
        "    \"\"\"\n",
        "    LangGraph node that creates and validates agent functions dynamically.\n",
        "    \"\"\"\n",
        "    print(\"---Dynamic Agent Function Creation & Validation---\")\n",
        "\n",
        "    tool_plan = state.get(\"tool_plan\", {})\n",
        "    created_functions = {}\n",
        "\n",
        "    # Prepare validation state with test data\n",
        "    validation_state = {\n",
        "        \"test_audio_dir\": state.get(\"test_audio_dir\", \"\"),\n",
        "        \"test_transcript_csv\": state.get(\"test_transcript_csv\", \"\"),\n",
        "        \"audio_dir\": state.get(\"audio_dir\", \"\"),\n",
        "        \"ground_truth_csv\": state.get(\"ground_truth_csv\", \"\"),\n",
        "        \"lang_code\": state.get(\"lang_code\", \"en\"),\n",
        "        \"csv_path\": state.get(\"test_transcript_csv\", \"\")  # For functions expecting csv_path\n",
        "    }\n",
        "\n",
        "    print(f\"Using validation state: {list(validation_state.keys())}\")\n",
        "\n",
        "    # Process each task in the plan\n",
        "    for plan_item in tool_plan.get(\"PLAN\", []):\n",
        "        task_name = plan_item.get(\"task\")\n",
        "        create_new_tasks = plan_item.get(\"create_new\", [])\n",
        "\n",
        "        print(f\"\\\\n📋 Processing task: {task_name}\")\n",
        "\n",
        "        for new_task in create_new_tasks:\n",
        "            function_name = new_task.get(\"name\")\n",
        "\n",
        "            # Create and validate the function\n",
        "            creation_result = create_and_validate_agent_function(new_task, validation_state)\n",
        "            created_functions[function_name] = creation_result\n",
        "\n",
        "    return {\n",
        "        \"created_agent_functions\": created_functions,\n",
        "        \"validation_state_used\": validation_state\n",
        "    }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
